---
layout: post
title: "I think ChatGPT's Deep Research will transform the process of doing secondary research"
---

At work, we tested ChatGPT Deep Research to produce a research report. For the prompt, we used the same prompt that I recently followed to produce my own research report. The prompt involved a detailed review of many specific questions relating to European aquaculture and fish markets. My own report is still unpublished, so ChatGPT does not have access to it. I didn't refine the prompt at all after seeing ChatGPT's report.  

(I haven't linked to either report here. Mine is still not publicly available, so it'd be hard to compare. In this post, I just want to write about my conclusions from this exercise.)  

TL;DR:
- This technology is freaking insane. The report took 11 minutes and it seems like it's at the level of a mid-quality professional research analyst. Compare this to 4 weeks for my own report.
- There are many reasons to believe it will continue to improve significantly, and many of the limitations seem readily fixable.
- I think that research will look very, very different in a decade's time.
- I'm going to explore alternative careers. I still think research is worth doing, and research skills are still worth having; if anything, the ability to distinguish truth from not-truth will become more important. But this new form of research could involve much heavier use of generative AI, and my heart isn't in it. I'd rather spend my time on other activities.

Detailed thoughts:

I read through the report generated by ChatGPT Deep Research itself and in terms of accuracy, it seems comparable to a typical report by a colleague I might be asked to review - it seems mostly correct, with a few details that I think are wrong but are really arguable either way, plus it has some insights I didn't include in mine.

It seems like generative AI is getting quite close to doing what we do - secondary research involving flexible judgment and aggregating a variety of sources. The bottleneck is human judgment stuff (e.g. some guy on the ground in Chile just told me XYZ, and I need to integrate this into my decision) and access to data (e.g. the technical and legal hurdles which are currently preventing it from scraping and using all scientific papers but could be overcome very easily). Of course, interacting with the physical world will remain a challenge for AI. But it's rare that we do original research at my current organisation - in almost all cases, we're doing secondary research using publicly available information (it's different for primary researchers/experimental scientists). I think it's already better than the lower-quality researchers who are currently working in research globally (not animal advocacy necessarily). Also need to take into account the fact that this took 11 minutes, compared to my systematic review report which took 4 weeks (and I'm pretty sure even that is more efficient than the average researcher).

It seems to me that the next few iterations of this Deep Research functionality will overtake human researchers very quickly. I remember playing around with GPT back when nerds knew about it but before it was easily publicly accessible (that is, pre-ChatGPT release). I think I was playing around with it when I was in the airport to EAGx Australia 2022. So around September 2022, or 2.5 years ago. The gap in the quality of output between that iteration and ChatGPT Deep Research is enormous. If you had shown me ChatGPT Deep Research then and said "Conditional on this being possible within the next 3 years, how likely is it that GPT will be better than any human at research within a further 5 years or whatever", I probably would have assigned that outcome a high likelihood. And that's the world we're now in.

So there are still a few bottlenecks and shortcomings. But I'm not aware of any specific reason why those bottlenecks and shortcomings won't be addressed - beyond the next iterations of GPT itself and then overcoming the technical/legal hurdles, it seems more likely to me than not that LLMs will outpace human researchers within like 3 years.

Which raises the question of what the future of research will look like. Even a hyperintelligent GPT wouldn't be something we could simply defer to in all decisions at my organisation (Animal Ask), for instance. We'd need at least a single human decision maker. But maybe the model of a typical organisation is more like "1 nerd + GPT" rather than "4 nerds".
